{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"gse-kUY0X07u"},"outputs":[],"source":["import pdb\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9h1iopcJieWm"},"outputs":[],"source":["def reset_seed():\n","    torch.manual_seed(42)\n","    random.seed(42)\n","    torch.cuda.manual_seed(42)"]},{"cell_type":"markdown","metadata":{"id":"tA-fDGW2gyQp"},"source":["# Custom Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x9TT7zf-Pvox"},"outputs":[],"source":["import torch.nn as nn\n","from torch.autograd import Variable\n","from torchvision import transforms\n","from torch.utils.data.dataset import Dataset\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torchvision\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","import os\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import numpy as np\n","\n","class QU_Dataset(Dataset):\n","  def __init__(self, image_dir, mask_dir, transform=None, state=None):\n","    self.image_dir = image_dir\n","    self.mask_dir = mask_dir\n","    self.transform = transform\n","    self.images = os.listdir(image_dir)\n","\n","  def __len__(self):\n","    return len(self.images)\n","\n","  def __getitem__(self, index):\n","    img_path = os.path.join(self.image_dir, self.images[index])\n","    mask_path = os.path.join(self.mask_dir, self.images[index]).replace(\".bmp\",\"_anno.bmp\")\n","    image = np.array(Image.open(img_path).convert(\"RGB\"))\n","    mask = np.array(Image.open(mask_path))\n","    mask[mask >= 1] = 1.0\n","\n","    if self.transform is not None:\n","      augmentations = self.transform(image=image, mask=mask)\n","      image = augmentations[\"image\"]\n","      mask = augmentations[\"mask\"]\n","    return image, mask\n","class DataScienceBowl(Dataset):\n","  def __init__(self, image_dir, mask_dir, transform=None):\n","    self.image_dir = image_dir\n","    self.mask_dir = mask_dir\n","    self.image = np.load(image_dir)\n","    self.image = np.expand_dims(self.image, axis = -1)\n","    self.image = np.concatenate((self.image, self.image, self.image), axis = -1)\n","    self.mask = np.load(mask_dir).astype(np.double)\n","    self.transform = transform\n","\n","  def __len__(self):\n","    return self.image.shape[0]\n","\n","  def __getitem__(self, index):\n","    image = self.image[index]*255\n","    mask = self.mask[index].squeeze()\n","\n","    if self.transform is not None:\n","      augmentations = self.transform(image=image, mask=mask)\n","      image = augmentations[\"image\"]\n","      mask = augmentations[\"mask\"]\n","    return image[0,:,:].unsqueeze(0), mask\n","IMAGE_HEIGHT = 256\n","IMAGE_WIDTH = 256\n","train_transform = A.Compose(\n","    [\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Rotate(limit=20, p=1.0),\n","        A.HorizontalFlip(p=0.5),\n","        A.VerticalFlip(p=0.1),\n","        A.Normalize(\n","            #mean = (0.485, 0.456, 0.406),\n","            #std = (0.229, 0.224, 0.225),\n","            mean = (0., 0., 0.),\n","            std = (1., 1., 1.),\n","            max_pixel_value = 255.0\n","        ),\n","        ToTensorV2(),\n","    ])\n","val_transform = A.Compose(\n","    [\n","        #A.RandomCrop(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Resize(height=IMAGE_HEIGHT, width=IMAGE_WIDTH),\n","        A.Normalize(\n","            mean = (0., 0., 0.),\n","            std = (1., 1., 1.),\n","            max_pixel_value = 255.0\n","        ),\n","        ToTensorV2(),\n","    ])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"75sO4L-zj-S4"},"outputs":[],"source":["reset_seed()\n","BATCH_SIZE = 16\n","NUM_WORKERS = 0\n","train_loader = DataLoader(\n","    train_ds,\n","    batch_size = BATCH_SIZE,\n","    num_workers = NUM_WORKERS,\n","    shuffle=True,\n",")\n","\"\"\"\n","val_loader = DataLoader(\n","    val_ds,\n","    \n","    batch_size=BATCH_SIZE,\n","    num_workers = NUM_WORKERS,\n","    pin_memory = PIN_MEMORY,\n","    shuffle=False\n",")\n","\"\"\"\n","test_loader = DataLoader(\n","    test_ds,\n","    batch_size=BATCH_SIZE,\n","    num_workers = 0,\n","    shuffle=False,\n",")"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"YH2f318e5h86"},"source":["#AMG Mixer"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5309,"status":"ok","timestamp":1673960627620,"user":{"displayName":"Minh Quang Lê Hoàng","userId":"07289054064054334657"},"user_tz":-420},"id":"__8Ppu8TcLVp","outputId":"12dea36e-e313-460f-8dcf-ff0ae351c264"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting einops\n","  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 KB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: einops\n","Successfully installed einops-0.6.0\n"]}],"source":["!pip install einops"]},{"cell_type":"markdown","metadata":{"id":"wtQC4ZxbsmDG"},"source":["##Axial attention"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5611,"status":"ok","timestamp":1673960633220,"user":{"displayName":"Minh Quang Lê Hoàng","userId":"07289054064054334657"},"user_tz":-420},"id":"H3vq1wMv8gU8","outputId":"09c5ae8b-250b-4c59-e427-3b312648579f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting timm\n","  Downloading timm-0.6.12-py3-none-any.whl (549 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 KB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.8/dist-packages (from timm) (0.14.0+cu116)\n","Requirement already satisfied: torch>=1.7 in /usr/local/lib/python3.8/dist-packages (from timm) (1.13.0+cu116)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.8/dist-packages (from timm) (6.0)\n","Collecting huggingface-hub\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=1.7->timm) (4.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (3.9.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (21.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (2.25.1)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from huggingface-hub->timm) (4.64.1)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (1.21.6)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.8/dist-packages (from torchvision->timm) (7.1.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (4.0.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->huggingface-hub->timm) (1.24.3)\n","Installing collected packages: huggingface-hub, timm\n","Successfully installed huggingface-hub-0.11.1 timm-0.6.12\n"]}],"source":["pip install timm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5srpMvlG8jQU"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.utils.checkpoint as checkpoint\n","from timm.models.layers import DropPath, to_2tuple, trunc_normal_"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VK6WTsLJP2D0"},"outputs":[],"source":["class PatchEmbed(nn.Module):\n","    r\"\"\" Image to Patch Embedding\n","    Args:\n","        img_size (int): Image size.  Default: 224.\n","        patch_size (int): Patch token size. Default: 4.\n","        in_chans (int): Number of input image channels. Default: 3.\n","        embed_dim (int): Number of linear projection output channels. Default: 96.\n","        norm_layer (nn.Module, optional): Normalization layer. Default: None\n","    \"\"\"\n","\n","    def __init__(self, img_size=224, patch_size=4, in_chans=3, embed_dim=96, norm_layer=None):\n","        super().__init__()\n","        img_size = to_2tuple(img_size)\n","        patch_size = to_2tuple(patch_size)\n","        patches_resolution = [img_size[0] // patch_size[0], img_size[1] // patch_size[1]]\n","        self.img_size = img_size\n","        self.patch_size = patch_size\n","        self.patches_resolution = patches_resolution\n","        self.num_patches = patches_resolution[0] * patches_resolution[1]\n","\n","        self.in_chans = in_chans\n","        self.embed_dim = embed_dim\n","\n","        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)\n","        if norm_layer is not None:\n","            self.norm = norm_layer(embed_dim)\n","        else:\n","            self.norm = None\n","\n","    def forward(self, x):\n","        B, C, H, W = x.shape\n","        # FIXME look at relaxing size constraints\n","        assert H == self.img_size[0] and W == self.img_size[1], \\\n","            f\"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]}).\"\n","        x = self.proj(x)#.flatten(2).transpose(1, 2)  # B Ph*Pw C\n","        if self.norm is not None:\n","            x = self.norm(x)\n","        return x\n","\n","    def flops(self):\n","        Ho, Wo = self.patches_resolution\n","        flops = Ho * Wo * self.embed_dim * self.in_chans * (self.patch_size[0] * self.patch_size[1])\n","        if self.norm is not None:\n","            flops += Ho * Wo * self.embed_dim\n","        return flops"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KGAT_ZWV0JeR"},"outputs":[],"source":["from einops import rearrange, reduce\n","import math\n","import torch.utils.model_zoo as model_zoo\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torchvision import transforms\n","from torch.utils.data.dataset import Dataset\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import torchvision\n","# from src.modules.eprop import eprop\n","import torch.utils.model_zoo as model_zoo\n","# from scripts.SEAM.network import resnet38_SEAM, resnet38_aff\n","from torch.utils.data import DataLoader\n","from PIL import Image\n","import os\n","import numpy as np\n","\n","# class SeparableConv2d(nn.Module):\n","#     def __init__(self, inplanes, planes, kernel_size=3, stride=2, dilation=2, bias=False):\n","#         super().__init__()\n","#         self.conv1 = nn.Conv2d(inplanes, inplanes, kernel_size, stride, padding=dilation, dilation=dilation,bias=bias)\n","#         self.bn = nn.BatchNorm2d(inplanes)\n","#         self.pointwise = nn.Conv2d(inplanes, planes, 1, 1, 0, 1, 1, bias=bias)\n","#         self.silu = nn.SiLU(inplace=True)\n","#     def forward(self, x):\n","#         x = self.conv1(x)\n","#         #x = self.silu(x)\n","#         x = self.bn(x)\n","#         x = self.pointwise(x)\n","#         return x\n","\n","def conv1x1(in_planes, out_planes, stride=1):\n","    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n","\n","class qkv_transform(nn.Conv1d):\n","  \"\"\"Conv1d for qkv_transform\"\"\"\n","\n","class AxialAttention(nn.Module):\n","    \n","    def __init__(self,\n","               in_planes,\n","               out_planes,\n","               groups=8,\n","               kernel_size=56,\n","               stride=1,\n","               bias=False,\n","               width=False,\n","               sep=False,\n","               ):\n","        assert (in_planes % groups == 0) and (out_planes % groups == 0)\n","        super().__init__()\n","        self.in_planes = in_planes\n","        self.out_planes = out_planes\n","        self.groups = groups\n","        self.group_planes = out_planes // groups\n","        self.kernel_size = kernel_size\n","        self.stride = stride\n","        self.bias = bias\n","        self.width = width\n","\n","        # Multi-head self attention\n","        self.qkv_transform = qkv_transform(in_planes, out_planes * 2, kernel_size=1, stride=1,\n","                                           padding=0, bias = False) # because Batchnorm\n","        self.bn_qkv = nn.BatchNorm1d(out_planes * 2)\n","        self.bn_similarity = nn.BatchNorm2d(groups * 3)\n","\n","        self.bn_output = nn.BatchNorm1d(out_planes * 2)\n","\n","        # Position embedding\n","        self.relative = nn.Parameter(torch.rand(self.group_planes * 2, kernel_size * 2 - 1), requires_grad=True)\n","        query_index = torch.arange(kernel_size).unsqueeze(0)\n","        key_index = torch.arange(kernel_size).unsqueeze(1)\n","        relative_index = key_index - query_index + kernel_size - 1\n","        self.register_buffer('flatten_index', relative_index.view(-1))\n","        if stride > 1:\n","            if sep :\n","                self.pooling = SeparableConv2d(out_planes, out_planes)\n","            else:\n","                self.pooling = nn.AvgPool2d(stride, stride=stride)\n","\n","        self.reset_parameters()\n","\n","    def forward(self, x):\n","        if self.width:\n","            x = x.permute(0, 2, 1, 3)\n","        else :\n","            x = x.permute(0, 3, 1, 2)\n","        N, W, C, H = x.shape\n","        x = x.contiguous().view(N*W, C, H)\n","\n","        # Transformations\n","        qkv = self.bn_qkv(self.qkv_transform(x))\n","        q, k, v = torch.split(qkv.reshape(N*W, self.groups, self.group_planes*2, H), \n","                              [self.group_planes // 2,self.group_planes // 2, self.group_planes], dim=2)\n","\n","        # Calculate position embedding\n","        all_embeddings = torch.index_select(self.relative, 1, self.flatten_index).view(self.group_planes*2, self.kernel_size, self.kernel_size )\n","        q_embedding, k_embedding, v_embedding = torch.split(all_embeddings, [self.group_planes // 2,self.group_planes // 2, self.group_planes],\n","                                                            dim = 0)\n","\n","        qr = torch.einsum('bgci,cij->bgij', q, q_embedding)\n","        kr = torch.einsum('bgcj,cij->bgij',k, k_embedding)\n","        qk = torch.einsum('bgci, bgcj -> bgij',q , k)\n","\n","        stacked_similarity = torch.cat([qk, qr, kr], dim=1)\n","        # bn_similarity chanels dim = self.group * 3\n","        stacked_similarity = self.bn_similarity(stacked_similarity).view(N*W, 3, self.groups, H, H).sum(dim=1)\n","        similarity = F.softmax(stacked_similarity, dim=3)\n","        sv = torch.einsum(\"bgij,bgci->bgci\", similarity, v)\n","        sve = torch.einsum(\"bgij,cij->bgci\", similarity, v_embedding)\n","        stacked_output = torch.cat([sv, sve], dim=-1).view(N*W, self.out_planes*2, H)\n","        output = self.bn_output(stacked_output).view(N, W, self.out_planes, 2, H).sum(dim=-2)\n","\n","        if self.width:\n","            output = output.permute(0, 2, 1, 3)\n","        else:\n","            output = output.permute(0, 2, 3, 1)\n","        if self.stride > 1:\n","            output = self.pooling(output)\n","        return output\n","  \n","    def reset_parameters(self):\n","        self.qkv_transform.weight.data.normal_(0, math.sqrt(1. / self.in_planes))\n","        nn.init.normal_(self.relative, 0, math.sqrt(1. / self.group_planes))\n","\n","class AxialTokenMix(nn.Module):\n","    # expansion = 1\n","\n","    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=8,\n","                 base_width=64, dilation=1, norm_layer=None, kernel_size=56):\n","        super(AxialTokenMix, self).__init__()\n","        if norm_layer is None:\n","            norm_layer = nn.BatchNorm2d\n","        width = int(planes * (base_width / 64.))\n","        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n","        self.conv_down = conv1x1(inplanes, width)\n","        self.bn1 = norm_layer(width)\n","        # height and weight axial attention\n","        self.hight_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size)\n","        self.width_block = AxialAttention(width, width, groups=groups, kernel_size=kernel_size, stride=stride, width=True)\n","        #squeeze and excitation\n","\n","        # self.prj1 = nn.Linear(width,width//2)\n","        # self.prj2 = nn.Linear(width//2, width)\n","        # self.prjout = nn.Linear(width, width)\n","\n","        #MBConv\n","\n","        self.mbconv = MBConv(width, width, 2, 3, 1, True, 0.25)\n","        \n","        # self.conv_up = conv1x1(width, planes * self.expansion)\n","        self.bn2 = norm_layer(planes)\n","        self.relu = nn.ReLU(inplace=True)\n","        self.sigmoid = nn.Sigmoid()\n","        self.downsample = downsample\n","        self.stride = stride\n","\n","    def forward(self, x):\n","        identity = x\n","\n","        out = self.conv_down(x)\n","        #print(out.shape)\n","        out = self.bn1(out)\n","        out = self.relu(out)\n","\n","        out = self.hight_block(out)\n","        # print(out.shape)\n","        out = self.width_block(out)\n","        # print(out.shape)\n","        out = self.relu(out)\n","        #squeeze and excitation block\n","\n","        # k = reduce(out, 'b c w h -> b 1 1 c', 'mean')\n","        # # print(k.shape)\n","        # a = self.relu(self.prj1(out.permute(0,2,3,1)))\n","        # # print(a.shape)\n","        # a = self.relu(self.prj2(a))\n","        # # print(a.shape)\n","        # a = self.sigmoid(a)\n","        # # print(a.shape)\n","        # x = self.prjout(k*a) + x.permute(0,2,3,1)\n","        # print(x.shape)\n","\n","        out = self.mbconv(out)\n","\n","        # out = self.conv_up(x.permute(0,3,1,2))\n","        out = self.bn2(out)\n","\n","        if self.downsample is not None:\n","            identity = self.downsample(x)\n","\n","        out += identity\n","        out = self.relu(out)\n","        # print(out.shape)\n","\n","        return out\n"]},{"cell_type":"markdown","metadata":{"id":"qgGkTfOpsraX"},"source":["##ConvBlock"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeaE71Gy68jn"},"outputs":[],"source":["from torch.nn.modules.pooling import MaxPool2d\n","class ConvBlock(nn.Module):\n","  def __init__(self, int , out):\n","    super().__init__()\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(int, out, kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(out),\n","        nn.ReLU(),\n","        #nn.MaxPool2d(2)\n","    )\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(out, out, kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(out),\n","        nn.ReLU(),\n","        #nn.MaxPool2d(2)\n","    )\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NMYHZN0-ImoF"},"outputs":[],"source":["from torch.nn.modules.pooling import MaxPool2d\n","class ConvBlockpool(nn.Module):\n","  def __init__(self, int , out):\n","    super().__init__()\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(int, out, kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(out),\n","        nn.ReLU(),\n","    )\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(out, out, kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(out),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2,2)\n","    )\n","  def forward(self, x):\n","    x = self.conv1(x)\n","    x = self.conv2(x)\n","    return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"O65q7POkyD-x"},"outputs":[],"source":["class feature_conv(nn.Module):\n","  def __init__(self, inchannels ):\n","    super().__init__()\n","    self.conv1 = ConvBlockpool(inchannels , 64)\n","    self.conv2 = ConvBlockpool(64, 128)\n","    self.conv3 = ConvBlockpool(128, 256)\n","  def forward(self, x):\n","    x1 = self.conv1(x)\n","    x2 = self.conv2(x1)\n","    x3= self.conv3(x2)\n","    return x1, x2 , x3 "]},{"cell_type":"markdown","metadata":{"id":"Ma0DlwKvsuf0"},"source":["##PASPP module"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7fhML-NLThtE"},"outputs":[],"source":["import torch.nn as nn\n","import torchvision\n","import torch\n","from skimage import morphology as morph\n","#from src.modules.eprop import eprop\n","import torch.utils.model_zoo as model_zoo\n","#from scripts.SEAM.network import resnet38_SEAM, resnet38_aff\n","import numpy as np\n","from torch import optim\n","import torch.nn.functional as F\n","\n","import torch\n","import torch.nn.functional as F\n","import numpy as np\n","from skimage.morphology import watershed\n","from skimage.segmentation import find_boundaries\n","from scipy import ndimage\n","\n","class _ASPPModule(nn.Module):\n","    def __init__(self, inplanes, planes, kernel_size, padding, dilation, BatchNorm):\n","        super().__init__()\n","        self.atrous_conv = nn.Conv2d(inplanes, planes, kernel_size=kernel_size,\n","                                      stride=1, padding=padding, dilation=dilation, bias=False)\n","        self.bn = BatchNorm(planes)\n","        self.silu = nn.SiLU(inplace=True)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        x = self.atrous_conv(x)\n","        x = self.bn(x)\n","\n","        return self.silu(x)\n","\n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class ASPP(nn.Module):\n","    def __init__(self, inplanes, outplanes, output_stride, BatchNorm):\n","        super().__init__()\n","        if output_stride == 4:\n","            dilations = [1, 6, 12, 18]\n","        elif output_stride == 8:\n","            dilations = [1, 4, 6, 10]\n","        elif output_stride == 2:\n","            dilations = [1, 12, 24, 36]\n","        else:\n","            raise NotImplementedError\n","\n","        #self.aspp1 = _ASPPModule(inplanes, outplanes, 1, padding=0,dilation=dilations[0], BatchNorm=BatchNorm)\n","        self.aspp2 = _ASPPModule(inplanes, outplanes, 3, padding=dilations[1], dilation=dilations[1], BatchNorm=BatchNorm)\n","        self.aspp3 = _ASPPModule(inplanes, outplanes, 3, padding=dilations[2], dilation=dilations[2], BatchNorm=BatchNorm)\n","        self.aspp4 = _ASPPModule(inplanes, outplanes, 3, padding=dilations[3], dilation=dilations[3], BatchNorm=BatchNorm)\n","\n","        self.global_avg_pool = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n","                                             nn.Conv2d(inplanes, outplanes, 1, stride=1, bias=False),\n","                                             #BatchNorm(outplanes),\n","                                             nn.SiLU(inplace=True))\n","        self.conv1 = nn.Conv2d(outplanes*4, outplanes, 1, bias=False)\n","        self.bn1 = BatchNorm(outplanes)\n","        self.silu = nn.SiLU(inplace=True)\n","        self.dropout = nn.Dropout(0.0)\n","        self._init_weight()\n","\n","    def forward(self, x):\n","        #x1 = self.aspp1(x)\n","        x2 = self.aspp2(x)\n","        x3 = self.aspp3(x)\n","        x4 = self.aspp4(x)\n","        x5 = self.global_avg_pool(x)\n","        x5 = F.interpolate(x5, size=x4.size()[2:], mode='bilinear', align_corners=True)\n","        x = torch.cat((x2, x3, x4, x5), dim=1)\n","\n","        x = self.conv1(x)\n","        x = self.bn1(x)\n","        x = self.silu(x)\n","\n","        return self.dropout(x)\n","  \n","    def _init_weight(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                torch.nn.init.kaiming_normal_(m.weight)\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","\n","class PASPP(nn.Module):\n","    def __init__(self, inplanes, outplanes, output_stride=4, BatchNorm=nn.BatchNorm2d):\n","        super().__init__()\n","        if output_stride == 4:\n","            dilations = [1, 6, 12, 18]\n","        elif output_stride == 8:\n","            dilations = [1, 4, 6, 10]\n","        elif output_stride == 2:\n","            dilations = [1, 12, 24, 36]\n","        elif output_stride == 16:\n","            dilations = [1, 2, 3, 4]\n","        elif output_stride == 1:\n","            dilations = [1, 16, 32, 48]\n","        else:\n","            raise NotImplementedError\n","        self._norm_layer = BatchNorm\n","        self.silu = nn.SiLU(inplace=True)\n","        self.conv1 = self._make_layer(inplanes, inplanes // 4)\n","        self.conv2 = self._make_layer(inplanes, inplanes // 4)\n","        self.conv3 = self._make_layer(inplanes, inplanes // 4)\n","        self.conv4 = self._make_layer(inplanes, inplanes // 4)\n","        self.atrous_conv1 = nn.Conv2d(inplanes // 4, inplanes // 4, kernel_size=3, dilation=dilations[0], padding=dilations[0])\n","        self.atrous_conv2 = nn.Conv2d(inplanes // 4, inplanes // 4, kernel_size=3, dilation=dilations[1], padding=dilations[1])\n","        self.atrous_conv3 = nn.Conv2d(inplanes // 4, inplanes // 4, kernel_size=3, dilation=dilations[2], padding=dilations[2])\n","        self.atrous_conv4 = nn.Conv2d(inplanes // 4, inplanes // 4, kernel_size=3, dilation=dilations[3], padding=dilations[3])\n","        self.conv5 = self._make_layer(inplanes // 2, inplanes // 2)\n","        self.conv6 = self._make_layer(inplanes // 2, inplanes // 2)\n","        self.convout = self._make_layer(inplanes, inplanes)\n","    \n","    def _make_layer(self, inplanes, outplanes):\n","        layer = []\n","        layer.append(nn.Conv2d(inplanes, outplanes, kernel_size = 1))\n","        layer.append(self._norm_layer(outplanes))\n","        layer.append(self.silu)\n","        return nn.Sequential(*layer)\n","    \n","    def forward(self, X):\n","        x1 = self.conv1(X)\n","        x2 = self.conv2(X)\n","        x3 = self.conv3(X)\n","        x4 = self.conv4(X)\n","        \n","        x12 = torch.add(x1, x2)\n","        x34 = torch.add(x3, x4)\n","        \n","        x1 = torch.add(self.atrous_conv1(x1),x12)\n","        x2 = torch.add(self.atrous_conv2(x2),x12)\n","        x3 = torch.add(self.atrous_conv3(x3),x34)\n","        x4 = torch.add(self.atrous_conv4(x4),x34)\n","        \n","        x12 = torch.cat([x1, x2], dim = 1)\n","        x34 = torch.cat([x3, x4], dim = 1)\n","        \n","        x12 = self.conv5(x12)\n","        x34 = self.conv5(x34)\n","        x = torch.cat([x12, x34], dim=1)\n","        x = self.convout(x)\n","        return x "]},{"cell_type":"markdown","metadata":{"id":"-YWx7pkvtHgH"},"source":["##Skip Connection"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tV_8Fri7tKAO"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","'''pixel-level module'''\n","\n","\n","class PixLevelModule(nn.Module):\n","    def __init__(self, in_channels):\n","        super(PixLevelModule, self).__init__()\n","        self.middle_layer_size_ratio = 2 \n","        self.conv_avg = nn.Conv2d(in_channels, out_channels=in_channels, kernel_size=1, bias=False)\n","        self.relu_avg = nn.ReLU(inplace=True)\n","        self.conv_max = nn.Conv2d(in_channels, out_channels=in_channels, kernel_size=1, bias=False)\n","        self.relu_max = nn.ReLU(inplace=True)\n","        self.bottleneck = nn.Sequential(\n","            nn.Linear(3, 3 * self.middle_layer_size_ratio),  # 2, 2*self.\n","            nn.ReLU(inplace=True),\n","            nn.Linear(3 * self.middle_layer_size_ratio, 1)\n","        )\n","        self.conv_sig = nn.Sequential(\n","            nn.Conv2d(in_channels, in_channels, kernel_size=1, stride=1, padding=0, bias=True),\n","            nn.Sigmoid()\n","        )\n","        # self.up = nn.Upsample(scale_factor=scale_factor)\n","\n","    '''forward'''\n","\n","    def forward(self, x, patch):\n","        x = rearrange(x, 'b (p1 p2) c -> b c p1 p2', p1 = patch, p2 = patch)\n","        x_avg = self.conv_avg(x)  \n","        x_avg = self.relu_avg(x_avg) \n","        x_avg = torch.mean(x_avg, dim=1)\n","        x_avg = x_avg.unsqueeze(dim=1)\n","        x_max = self.conv_max(x)\n","        x_max = self.relu_max(x_max)\n","        x_max = torch.max(x_max, dim=1).values\n","        x_max = x_max.unsqueeze(dim=1)\n","        x_out = x_max+x_avg\n","        x_output = torch.cat((x_avg, x_max, x_out), dim=1) \n","        x_output = x_output.transpose(1, 3) \n","        x_output = self.bottleneck(x_output)\n","        x_output = x_output.transpose(1, 3) \n","        y = x_output * x\n","        return y"]},{"cell_type":"markdown","metadata":{"id":"lXj4P7UmuJe-"},"source":["##Decoder Upsample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJzauM9E7JS2"},"outputs":[],"source":["class DecoderUNit(nn.Module):\n","  def __init__(self, inchannels, outchannels, size):\n","    super().__init__()\n","    self.up = nn.Upsample(size = size)\n","    self.conv1 = nn.Sequential(\n","        nn.Conv2d(inchannels[0], inchannels[0] , kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(inchannels[0]),\n","        nn.ReLU(),\n","    )\n","        # nn.Upsample(size = size),\n","        # nn.Conv2d(256 + inchannels, outchannels),\n","        # nn.BatchNorm2d(outchannels),\n","        # nn.Relu(),\n","    self.conv2 = nn.Sequential(\n","        nn.Conv2d(inchannels[1], outchannels, kernel_size = 3, stride = 1, padding = 1),\n","        nn.BatchNorm2d(outchannels),\n","        nn.ReLU()      \n","    )\n","    # pass\n","  def forward(self, x, en = None, patch = None):\n","    if en is not None:\n","      # skip = self.skip(en, patch)\n","      # skip = rearrange(en, 'b (p1 p2) c -> b c p1 p2', p1 = patch, p2 = patch)\n","      x = x + en\n","      shortcut = x.clone()\n","      x = self.conv1(x)\n","      x = x + shortcut\n","    if en is not None:\n","      x = torch.cat([x,en], dim = 1 )\n","    x = self.up(x)\n","    x = self.conv2(x)\n","\n","    return x\n","\n","class PatchMerging(nn.Module):\n","    r\"\"\" Patch Merging Layer.\n","    Args:\n","        input_resolution (tuple[int]): Resolution of input feature.\n","        dim (int): Number of input channels.\n","        norm_layer (nn.Module, optional): Normalization layer.  Default: nn.LayerNorm\n","    \"\"\"\n","\n","    def __init__(self, input_resolution, dim, norm_layer=nn.LayerNorm):\n","        super().__init__()\n","        self.input_resolution = input_resolution\n","        self.dim = dim\n","        self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n","        self.norm = norm_layer(4 * dim)\n","\n","    def forward(self, x):\n","        \"\"\"\n","        x: B, H*W, C\n","        \"\"\"\n","        H, W = self.input_resolution\n","        B, L, C = x.shape\n","        assert L == H * W, \"input feature has wrong size\"\n","        assert H % 2 == 0 and W % 2 == 0, f\"x size ({H}*{W}) are not even.\"\n","\n","        x = x.view(B, H, W, C)\n","\n","        x0 = x[:, 0::2, 0::2, :]  # B H/2 W/2 C\n","        x1 = x[:, 1::2, 0::2, :]  # B H/2 W/2 C\n","        x2 = x[:, 0::2, 1::2, :]  # B H/2 W/2 C\n","        x3 = x[:, 1::2, 1::2, :]  # B H/2 W/2 C\n","        x = torch.cat([x0, x1, x2, x3], -1)  # B H/2 W/2 4*C\n","        x = x.view(B, -1, 4 * C)  # B H/2*W/2 4*C\n","\n","        x = self.norm(x)\n","        x = self.reduction(x)\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"wrUn9VfZOOkP"},"source":["##Multi axis attention\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uQAmUWJV4dPc"},"outputs":[],"source":["import torch.nn as nn\n","import torch\n","import numpy as np\n","import einops\n","\n","def block_images_einops(x, patch_size):\n","  \"\"\"Image to patches.\"\"\"\n","  batch, height, width, channels = x.shape\n","  grid_height = height // patch_size[0]\n","  grid_width = width // patch_size[1]\n","  x = einops.rearrange(\n","      x, \"n (gh fh) (gw fw) c -> n (gh gw) (fh fw) c\",\n","      gh=grid_height, gw=grid_width, fh=patch_size[0], fw=patch_size[1]) \n","  return x\n","\n","\n","def unblock_images_einops(x, grid_size, patch_size):\n","  \"\"\"patches to images.\"\"\"\n","  x = einops.rearrange(\n","      x, \"n (gh gw) (fh fw) c -> n (gh fh) (gw fw) c\",\n","      gh=grid_size[0], gw=grid_size[1], fh=patch_size[0], fw=patch_size[1])\n","  return x\n","\n","\n","# MFI\n","class GetSpatialGatingWeights_2D_Multi_Scale_Cascade_Grid(nn.Module):\n","    \"\"\"Get gating weights for cross-gating MLP block.\"\"\"\n","    def __init__(self,nIn:int,Nout:int,H_size:int=128,W_size:int=128,input_proj_factor:int=2,dropout_rate:float=0.0,use_bias:bool=True,train_size:int=512):\n","        super(GetSpatialGatingWeights_2D_Multi_Scale_Cascade_Grid, self).__init__()\n","        \n","        self.H = H_size\n","        self.W = W_size\n","        self.IN = nIn\n","        self.OUT = Nout\n","        if train_size == 512:\n","            self.grid_size = [[8, 8], [4, 4], [2, 2]]\n","        else:\n","            self.grid_size = [[6, 6], [3, 3], [2, 2]]\n","\n","        self.block_size = [[int(H_size / l[0]), int(W_size / l[1])] for l in self.grid_size]\n","        self.input_proj_factor = input_proj_factor\n","        self.dropout_rate = dropout_rate\n","        self.use_bias = use_bias\n","        self.dropout = nn.Dropout(self.dropout_rate)\n","        self.LayerNorm = nn.LayerNorm(self.IN)\n","        self.Linear_end = nn.Linear(self.IN,self.OUT)\n","        self.Gelu = nn.GELU()\n","        self.Linear_grid_MLP_1 = nn.Linear((self.grid_size[0][0]*self.grid_size[0][1]),(self.grid_size[0][0]*self.grid_size[0][1]),bias=use_bias)\n","\n","        self.Linear_Block_MLP_1 = nn.Linear((self.block_size[0][0]*self.block_size[0][1]),(self.block_size[0][0]*self.block_size[0][1]),bias=use_bias)\n","\n","        self.Linear_grid_MLP_2 = nn.Linear((self.grid_size[1][0] * self.grid_size[1][1]),\n","                                           (self.grid_size[1][0] * self.grid_size[1][1]), bias=use_bias)\n","\n","        self.Linear_Block_MLP_2 = nn.Linear((self.block_size[1][0] * self.block_size[1][1]),\n","                                            (self.block_size[1][0] * self.block_size[1][1]), bias=use_bias)\n","\n","        self.Linear_grid_MLP_3 = nn.Linear((self.grid_size[2][0] * self.grid_size[2][1]),\n","                                           (self.grid_size[2][0] * self.grid_size[2][1]), bias=use_bias)\n","\n","        self.Linear_Block_MLP_3 = nn.Linear((self.block_size[2][0] * self.block_size[2][1]),\n","                                            (self.block_size[2][0] * self.block_size[2][1]), bias=use_bias)\n","\n","    def forward(self, x): \n","        n, h, w,num_channels = x.shape\n","        \n","        x = self.LayerNorm(x.float()) \n","        x = self.Gelu(x)\n","\n","       \n","        gh1, gw1 = self.grid_size[0]\n","        fh1, fw1 = h // gh1, w // gw1\n","        u1 = block_images_einops(x, patch_size=(fh1, fw1))\n","        u1 = u1.permute(0,3,2,1)\n","\n","        u1 = self.Linear_grid_MLP_1(u1)\n","        u1 = u1.permute(0,3,2,1)\n","        u1 = unblock_images_einops(u1, grid_size=(gh1, gw1), patch_size=(fh1, fw1))\n","\n","        fh1, fw1 = self.block_size[0]\n","        gh1, gw1 = h // fh1, w // fw1\n","        v1 = block_images_einops(u1, patch_size=(fh1, fw1))\n","        v1 = v1.permute(0, 1, 3, 2)\n","        v1 = self.Linear_Block_MLP_1(v1)\n","        v1 = v1.permute(0, 1, 3, 2)\n","        v1 = unblock_images_einops(v1, grid_size=(gh1, gw1), patch_size=(fh1, fw1))\n","\n","        gh2, gw2 = self.grid_size[1]\n","        fh2, fw2 = h // gh2, w // gw2\n","        u2 = block_images_einops(v1, patch_size=(fh2, fw2)) \n","        u2 = u2.permute(0, 3, 2, 1)\n","\n","        u2 = self.Linear_grid_MLP_2(u2)\n","        u2 = u2.permute(0, 3, 2, 1)\n","        u2 = unblock_images_einops(u2, grid_size=(gh2, gw2), patch_size=(fh2, fw2))\n","\n","        fh2, fw2 = self.block_size[1]\n","        gh2, gw2 = h // fh2, w // fw2\n","        v2 = block_images_einops(u2, patch_size=(fh2, fw2))\n","        v2 = v2.permute(0, 1, 3, 2)\n","        v2 = self.Linear_Block_MLP_2(v2)\n","        v2 = v2.permute(0, 1, 3, 2)\n","        v2 = unblock_images_einops(v2, grid_size=(gh2, gw2), patch_size=(fh2, fw2))\n","\n","        gh3, gw3 = self.grid_size[2]\n","        fh3, fw3 = h // gh3, w // gw3\n","        u3 = block_images_einops(v2, patch_size=(fh3, fw3))  \n","        u3 = u3.permute(0, 3, 2, 1)\n","\n","        u3 = self.Linear_grid_MLP_3(u3)\n","        u3 = u3.permute(0, 3, 2, 1)\n","        u3 = unblock_images_einops(u3, grid_size=(gh3, gw3), patch_size=(fh3, fw3))\n","\n","        fh3, fw3 = self.block_size[2]\n","        gh3, gw3 = h // fh3, w // fw3\n","        v3 = block_images_einops(u3, patch_size=(fh3, fw3))\n","        v3 = v3.permute(0, 1, 3, 2)\n","        v3 = self.Linear_Block_MLP_3(v3)\n","        v3 = v3.permute(0, 1, 3, 2)\n","        v3 = unblock_images_einops(v3, grid_size=(gh3, gw3), patch_size=(fh3, fw3))\n","\n","        x = self.Linear_end(v3)\n","        x = self.dropout(x)\n","        return x\n","\n","\n","class conv_T_y_2_x(nn.Module):\n","    \"\"\" Unified y Dimensional to x \"\"\"\n","    def __init__(self,y_nIn,x_nOut):\n","        super(conv_T_y_2_x, self).__init__()\n","        self.x_c = x_nOut\n","        self.y_c = y_nIn\n","        self.convT = nn.ConvTranspose2d(in_channels=self.y_c, out_channels=self.x_c, kernel_size=(3,3),\n","                                        stride=(2, 2))\n","    def forward(self,x,y):\n","       \n","        y = self.convT(y)\n","        _, _, h, w, = x.shape\n","        y = nn.Upsample(size=(h, w), mode='bilinear', align_corners=True)(y)\n","        return y\n","\n","\n","\n","class CrossGatingBlock(nn.Module):\n","    \"\"\"Cross-gating MLP block.\"\"\"\n","    def __init__(self,x_in:int,y_in:int,out_features:int,patch_size:[int,int],block_size:[int,int],grid_size:[int,int],dropout_rate:float=0.0,input_proj_factor:int=2,upsample_y:bool=True,use_bias:bool=True, train_size:int=512):\n","        super(CrossGatingBlock, self).__init__()\n","        self.conv = nn.Sequential(\n","            nn.BatchNorm2d(out_features),\n","            nn.SiLU(inplace=True),\n","            nn.Conv2d(out_features, out_features, kernel_size=3, stride=1, padding=1, groups = out_features, bias=False),\n","            nn.Dropout(p=0.1), # save load thi bo Dropout\n","            nn.BatchNorm2d(out_features),\n","            nn.SiLU(inplace=True),\n","            nn.Conv2d(out_features, out_features, kernel_size=1)\n","        )\n","        self.IN_x = x_in\n","        self.IN_y = y_in\n","        self._h = patch_size[0]\n","        self._w = patch_size[1]\n","        self.features = out_features\n","        self.block_size=block_size\n","        self.grid_size = grid_size\n","        self.dropout_rate = dropout_rate\n","        self.input_proj_factor = input_proj_factor\n","        self.upsample_y = upsample_y\n","        self.use_bias = use_bias\n","        self.Conv1X1_x = nn.Conv2d(self.IN_x,self.features,(1,1))\n","        self.Conv1X1_y = nn.Conv2d(self.IN_x,self.features,(1,1))\n","        self.LayerNorm_x = nn.LayerNorm(self.features)\n","        self.LayerNorm_y = nn.LayerNorm(self.features)\n","        self.Linear_x = nn.Linear(self.features,self.features,bias=use_bias)\n","        self.Linear_y = nn.Linear(self.features,self.features,bias=use_bias)\n","        self.Gelu_x = nn.GELU()\n","        self.Gelu_y = nn.GELU()\n","        self.Linear_x_end = nn.Linear(self.features,self.features,bias=use_bias)\n","        self.Linear_y_end = nn.Linear(self.features,self.features,bias=use_bias)\n","        self.dropout_x = nn.Dropout(self.dropout_rate)\n","        self.dropout_y = nn.Dropout(self.dropout_rate)\n","\n","        self.ConvT = conv_T_y_2_x(self.IN_y,self.IN_x)\n","        self.fun_gx = GetSpatialGatingWeights_2D_Multi_Scale_Cascade_Grid(nIn=self.features, Nout=self.features, H_size=self._h, W_size=self._w,\n","                                                 input_proj_factor=2, dropout_rate=dropout_rate, use_bias=True, train_size=train_size)\n","\n","        self.fun_gy = GetSpatialGatingWeights_2D_Multi_Scale_Cascade_Grid(nIn=self.features, Nout=self.features, H_size=self._h, W_size=self._w,\n","                                                 input_proj_factor=2, dropout_rate=dropout_rate, use_bias=True, train_size=train_size)\n","\n","    def forward(self, x):\n","    # Upscale Y signal, y is the gating signal.\n","        # x = rearrange(x, 'b (p1 p2) c -> b c p1 p2', p1 = patch1, p2 = patch1)\n","        # ####\n","        # y = rearrange(y, 'b (p1 p2) c -> b c p1 p2', p1 = patch2, p2 = patch2)\n","        # if self.upsample_y:\n","           \n","        #     y = self.ConvT(x,y)\n","\n","        x = self.Conv1X1_x(x)\n","        # y = self.Conv1X1_y(y)\n","        # assert y.shape == x.shape\n","        x = x.permute(0, 2, 3, 1)  # n x h x w x c\n","        # y = y.permute(0, 2, 3, 1)\n","        shortcut_x = x\n","        # shortcut_y = y\n","        # Get gating weights from X\n","        x = self.LayerNorm_x(x)\n","        x = self.Linear_x(x)\n","        x = self.Gelu_x(x)\n","\n","        gx = self.fun_gx(x)\n","        # n x h x w x c\n","        # Get gating weights from Y\n","        # y = self.LayerNorm_y(y)\n","        # y = self.Linear_y(y)\n","        # y = self.Gelu_y(y)\n","\n","        # gy = self.fun_gy(y)\n","\n","        # y = y * gx\n","        # y = self.Linear_y_end(y)\n","        # y = self.dropout_y(y)\n","        # y = y + shortcut_y\n","        # x = x * gy  # gating x using y\n","        x = self.Linear_y_end(x)\n","        x = self.dropout_x(x)\n","        x = x  + shortcut_x  \n","        x = x.permute(0, 3, 1, 2)  # n x h x w x c --> n x c x h x w\n","        # y = y.permute(0, 3, 1, 2)\n","        # logit = torch.cat([x,y], dim = 1)\n","        x = self.conv(x)\n","        return x\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RU39bohd3Pfq"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","'''pixel-level module'''\n","\n","\n","class SkipConnection(nn.Module):\n","    def __init__(self, in_channels):\n","        super().__init__()\n","        # self.middle_layer_size_ratio = 1\n","        self.mlp = nn.Sequential(\n","            nn.Linear(in_channels, in_channels),\n","            nn.GELU()\n","        ) \n","        self.up = nn.Upsample(scale_factor = 2)\n","        self.conv = nn.Sequential(\n","            nn.Conv2d(in_channels*2, in_channels, kernel_size = 3, padding = 1, stride = 1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(in_channels),\n","            nn.Conv2d(in_channels, in_channels, kernel_size = 3, padding = 1, stride = 1),\n","            nn.ReLU(),\n","            nn.BatchNorm2d(in_channels),\n","        )\n","        # self.up = nn.Upsample(scale_factor=scale_factor)\n","\n","    '''forward'''\n","\n","    def forward(self, em1, em2, patch1, patch2):\n","        # x = rearrange(x, 'b (p1 p2) c -> b c p1 p2', p1 = patch, p2 = patch)\n","        x1 = self.mlp(em1)\n","        x1 = rearrange(x1, 'b (p1 p2) c -> b c p1 p2', p1 = patch1, p2 = patch1)\n","        ####\n","        x2 = self.mlp(em2)\n","        x2 = rearrange(x2, 'b (p1 p2) c -> b c p1 p2', p1 = patch2, p2 = patch2)\n","        x2 = self.up(x2)\n","        ####\n","        x = torch.cat([x1, x2], dim = 1)\n","        x = self.conv(x)\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"y8O5m0M2OFOB"},"outputs":[],"source":["import math\n","\n","import torch\n","import torch.nn as nn\n","\n","import math\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","\n","def conv_bn_act(in_, out_, kernel_size,\n","                stride=1, groups=1, bias=True,\n","                eps=1e-3, momentum=0.01):\n","    return nn.Sequential(\n","        SamePadConv2d(in_, out_, kernel_size, stride, groups=groups, bias=bias),\n","        nn.BatchNorm2d(out_, eps, momentum),\n","        Swish()\n","    )\n","\n","\n","class SamePadConv2d(nn.Conv2d):\n","    \"\"\"\n","    Conv with TF padding='same'\n","    https://github.com/pytorch/pytorch/issues/3867#issuecomment-349279036\n","    \"\"\"\n","\n","    def __init__(self, in_channels, out_channels, kernel_size, stride=1, dilation=1, groups=1, bias=True, padding_mode=\"zeros\"):\n","        super().__init__(in_channels, out_channels, kernel_size, stride, 0, dilation, groups, bias, padding_mode)\n","\n","    def get_pad_odd(self, in_, weight, stride, dilation):\n","        effective_filter_size_rows = (weight - 1) * dilation + 1\n","        out_rows = (in_ + stride - 1) // stride\n","        padding_needed = max(0, (out_rows - 1) * stride + effective_filter_size_rows - in_)\n","        padding_rows = max(0, (out_rows - 1) * stride + (weight - 1) * dilation + 1 - in_)\n","        rows_odd = (padding_rows % 2 != 0)\n","        return padding_rows, rows_odd\n","\n","    def forward(self, x):\n","        padding_rows, rows_odd = self.get_pad_odd(x.shape[2], self.weight.shape[2], self.stride[0], self.dilation[0])\n","        padding_cols, cols_odd = self.get_pad_odd(x.shape[3], self.weight.shape[3], self.stride[1], self.dilation[1])\n","\n","        if rows_odd or cols_odd:\n","            x = F.pad(x, [0, int(cols_odd), 0, int(rows_odd)])\n","\n","        return F.conv2d(x, self.weight, self.bias, self.stride,\n","                        padding=(padding_rows // 2, padding_cols // 2),\n","                        dilation=self.dilation, groups=self.groups)\n","\n","\n","class Swish(nn.Module):\n","    def forward(self, x):\n","        return x * torch.sigmoid(x)\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.shape[0], -1)\n","\n","\n","class SEModule(nn.Module):\n","    def __init__(self, in_, squeeze_ch):\n","        super().__init__()\n","        self.se = nn.Sequential(\n","            nn.AdaptiveAvgPool2d(1),\n","            nn.Conv2d(in_, squeeze_ch, kernel_size=1, stride=1, padding=0, bias=True),\n","            Swish(),\n","            nn.Conv2d(squeeze_ch, in_, kernel_size=1, stride=1, padding=0, bias=True),\n","        )\n","\n","    def forward(self, x):\n","        return x * torch.sigmoid(self.se(x))\n","\n","\n","class DropConnect(nn.Module):\n","    def __init__(self, ratio):\n","        super().__init__()\n","        self.ratio = 1.0 - ratio\n","\n","    def forward(self, x):\n","        if not self.training:\n","            return x\n","\n","        random_tensor = self.ratio\n","        random_tensor += torch.rand([x.shape[0], 1, 1, 1], dtype=torch.float, device=x.device)\n","        random_tensor.requires_grad_(False)\n","        return x / self.ratio * random_tensor.floor()\n","\n","class MBConv(nn.Module):\n","    def __init__(self, in_, out_, expand,\n","                 kernel_size, stride, skip,\n","                 se_ratio, dc_ratio=0.2):\n","        super().__init__()\n","        mid_ = in_ * expand\n","        self.expand_conv = conv_bn_act(in_, mid_, kernel_size=1, bias=False) if expand != 1 else nn.Identity()\n","\n","        self.depth_wise_conv = conv_bn_act(mid_, mid_,\n","                                           kernel_size=kernel_size, stride=stride,\n","                                           groups=mid_, bias=False)\n","\n","        self.se = SEModule(mid_, int(in_ * se_ratio)) if se_ratio > 0 else nn.Identity()\n","\n","        self.project_conv = nn.Sequential(\n","            SamePadConv2d(mid_, out_, kernel_size=1, stride=1, bias=False),\n","            nn.BatchNorm2d(out_, 1e-3, 0.01)\n","        )\n","\n","        # if _block_args.id_skip:\n","        # and all(s == 1 for s in self._block_args.strides)\n","        # and self._block_args.input_filters == self._block_args.output_filters:\n","        self.skip = skip and (stride == 1) and (in_ == out_)\n","\n","        # DropConnect\n","        # self.dropconnect = DropConnect(dc_ratio) if dc_ratio > 0 else nn.Identity()\n","        # Original TF Repo not using drop_rate\n","        # https://github.com/tensorflow/tpu/blob/05f7b15cdf0ae36bac84beb4aef0a09983ce8f66/models/official/efficientnet/efficientnet_model.py#L408\n","        self.dropconnect = nn.Identity()\n","\n","    def forward(self, inputs):\n","        expand = self.expand_conv(inputs)\n","        x = self.depth_wise_conv(expand)\n","        x = self.se(x)\n","        x = self.project_conv(x)\n","        if self.skip:\n","            x = self.dropconnect(x)\n","            x = x + inputs\n","        return x\n"]},{"cell_type":"markdown","metadata":{"id":"XattPByhs1gc"},"source":["##Build Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"N3eQbiqv5mZb"},"outputs":[],"source":["from torch import nn\n","from torch.nn import functional as F\n","\n","\n","class MLP(nn.Module):\n","    def __init__(self, num_features, expansion_factor, dropout):\n","        super().__init__()\n","        num_hidden = num_features * expansion_factor\n","        self.fc1 = nn.Linear(num_features, num_hidden)\n","        self.dropout1 = nn.Dropout(dropout)\n","        self.fc2 = nn.Linear(num_hidden, num_features)\n","        self.dropout2 = nn.Dropout(dropout)\n","\n","    def forward(self, x):\n","        x = self.dropout1(F.gelu(self.fc1(x)))\n","        x = self.dropout2(self.fc2(x))\n","        return x\n","\n","\n","class TokenMixer(nn.Module):\n","    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n","        super().__init__()\n","        self.num_patches = num_patches\n","        # self.shift = shiftmlp(num_features, num_features, num_features)\n","        #self.num_features = num_features\n","        self.norm = nn.LayerNorm(num_features)\n","        self.mlp = MLP(num_patches, expansion_factor, dropout)\n","        self.axialatt = AxialTokenMix(num_features, num_features, kernel_size = num_patches)\n","\n","    def forward(self, x):\n","        # x.shape == (batch_size, num_patches, num_features)\n","        residual = x\n","        # x = self.shift(x, self.num_patches, self.num_patches)\n","        x = self.norm(x)\n","        x = rearrange(x, 'b (p1 p2) c -> b c p1 p2', p1 = self.num_patches, p2 = self.num_patches )\n","        x = self.axialatt(x)\n","        # x = x.transpose(1, 2)\n","        # 4,256,16,16\n","        # x.shape == (batch_size, num_features, num_patches)\n","        #x = self.mlp(x)\n","        # x = x.transpose(1, 2)\n","        # x = x.view(batch_size, -1, self.num_features)\n","        # x.shape == (batch_size, num_patches, num_features)\n","        x = rearrange(x, 'b c p1 p2 -> b (p1 p2) c', p1 = self.num_patches, p2 = self.num_patches )\n","        out = x + residual\n","        return out\n","\n","\n","class ChannelMixer(nn.Module):\n","    def __init__(self, num_features, num_patches, expansion_factor, dropout):\n","        super().__init__()\n","        self.norm = nn.LayerNorm(num_features)\n","        self.mlp = MLP(num_features, expansion_factor, dropout)\n","\n","    def forward(self, x):\n","        # x.shape == (batch_size, num_patches, num_features)\n","        residual = x\n","        x = self.norm(x)\n","        x = self.mlp(x)\n","        # x.shape == (batch_size, num_patches, num_features)\n","        out = x + residual\n","        return out\n","\n","\n","class MixerLayer(nn.Module):\n","    def __init__(self, num_features, num_patches, sqrt_num_patches, expansion_factor, dropout):\n","        super().__init__()\n","        self.token_mixer = TokenMixer(\n","            num_features, sqrt_num_patches , expansion_factor, dropout\n","        )\n","        self.channel_mixer = ChannelMixer(\n","            num_features, num_patches, expansion_factor, dropout\n","        )\n","\n","    def forward(self, x):\n","        # x.shape == (batch_size, num_patches, num_features)\n","        x = self.token_mixer(x)\n","        x = self.channel_mixer(x)\n","        # x.shape == (batch_size, num_patches, num_features)\n","        return x\n","\n","\n","def check_sizes(image_size, patch_size):\n","    sqrt_num_patches, remainder = divmod(image_size, patch_size)\n","    assert remainder == 0, \"`image_size` must be divisibe by `patch_size`\"\n","    num_patches = sqrt_num_patches ** 2\n","    return sqrt_num_patches, num_patches\n","\n","class MLPMixer(nn.Module):\n","    def __init__(\n","        self,\n","        image_size=256,\n","        patch_size=4,\n","        in_channels=1,\n","        num_features=64,\n","        expansion_factor=2,\n","        num_layers = [2,2,6,2],\n","        num_classes=2,\n","        dropout=0.1\n","    ):  \n","        self.sqrt_num_patches, self.num_patches = check_sizes(image_size, patch_size)\n","        super().__init__()\n","        self.patch_size = patch_size\n","        # per-patch fully-connected is equivalent to strided conv2d\n","        self.patcher = nn.Conv2d(\n","            in_channels, num_features, kernel_size=patch_size, stride=patch_size\n","        )###???\n","        # self.z = []\n","        self.mixers0 = nn.Sequential(\n","            *[  \n","                # for _ in range(num_layers)\n","                MixerLayer(num_features, self.num_patches,  self.sqrt_num_patches, expansion_factor, dropout)\n","                for _ in range(num_layers[0])\n","            ]\n","        )\n","        self.patch_merge0 = PatchMerging([64, 64], 64)\n","        self.mixers1 = nn.Sequential(\n","            *[  \n","                # for _ in range(num_layers)\n","                MixerLayer(num_features*2, self.num_patches//4,  self.sqrt_num_patches//2, expansion_factor, dropout)\n","                for _ in range(num_layers[1])\n","            ]\n","        )\n","        self.patch_merge1 = PatchMerging([32, 32], 128)\n","        self.mixers2 = nn.Sequential(\n","            *[  \n","                # for _ in range(num_layers)\n","                MixerLayer(num_features*4, self.num_patches//16,  self.sqrt_num_patches//4, expansion_factor, dropout)\n","                for _ in range(num_layers[2])\n","            ]\n","        )\n","        self.patch_merge2 = PatchMerging([16, 16], 256)\n","        self.mixers3 = nn.Sequential(\n","            *[  \n","                # for _ in range(num_layers)\n","                MixerLayer(num_features*8, self.num_patches//32,  self.sqrt_num_patches//8, expansion_factor, dropout)\n","                for _ in range(num_layers[3])\n","            ]\n","        )\n","        self.paspp = PASPP(512,512, output_stride = 4)\n","        # self.fea_conv = feature_conv(in_channels)\n","        # self.classifier = nn.Linear(num_features, num_classes)\n","        # self.skip4 = CrossGatingBlock(512, 512, 512, [8, 8], [8, 8], [4, 4], 0.1, upsample_y=False, train_size=512)\n","        self.skip3 = CrossGatingBlock(256, 256, 256, [16, 16], [8, 8], [4, 4], 0.1, upsample_y=False, train_size=512)\n","        self.skip2 = CrossGatingBlock(128, 128, 128, [32, 32], [8, 8], [4, 4], 0.1, upsample_y=False, train_size=512)\n","        self.skip1 = CrossGatingBlock(64, 64, 64,[64, 64], [8, 8], [4, 4], 0.1, upsample_y=False, train_size=512)\n","        self.conv_last = nn.Sequential(\n","            nn.Upsample(scale_factor=2),\n","            ConvBlock(32, num_classes)\n","            # nn.Conv2d(32, num_classes, 1)\n","        )\n","        self.du1 = DecoderUNit([512, 512], 256, 16)\n","        self.du2 = DecoderUNit([256, 512], 128, 32)\n","        self.du3 = DecoderUNit([128, 256 ], 64, 64)\n","        self.du4 = DecoderUNit([64, 128 ], 32, 128)\n","\n","  \n","    def forward(self, x):\n","        # x1, x2, x3 = self.fea_conv(x)\n","        patches = self.patcher(x)\n","        batch_size, num_features, _ , _ = patches.shape\n","        patches = patches.permute(0, 2, 3, 1)\n","        patches = patches.view(batch_size, -1, num_features)\n","        # patches.shape == (batch_size, num_patches, num_features)\n","        \n","        #old embedding\n","        # embedding = self.mixers(patches)\n","\n","        #new embedding:\n","        # stage 1\n","        embedding0 = self.mixers0(patches)# 64 64\n","        embedding1 = self.patch_merge0(embedding0)\n","        # print(embedding0.shape)\n","        # stage 2\n","        embedding1 = self.mixers1(embedding1)# 32 32\n","        embedding2 = self.patch_merge1(embedding1)\n","        # print(embedding1.shape)\n","        # stage 3\n","        embedding2 = self.mixers2(embedding2)# 16 16\n","        embedding3 = self.patch_merge2(embedding2)\n","        # print(embedding2.shape)\n","        # stage 4\n","        embedding = self.mixers3(embedding3)#8 8\n","        # print(embedding.shape)\n","        # embedding = self.lastmlp(embedding)\n","        embedding = rearrange(embedding, 'b (p1 p2) c -> b c p1 p2', p1 = self.sqrt_num_patches//8, p2 = self.sqrt_num_patches//8 )\n","        embedding = self.paspp(embedding)\n","\n","        embedding2 = rearrange(embedding2, 'b (p1 p2) c -> b c p1 p2', p1 = self.sqrt_num_patches//4, p2 = self.sqrt_num_patches//4 )\n","        embedding1 = rearrange(embedding1, 'b (p1 p2) c -> b c p1 p2', p1 = self.sqrt_num_patches//2, p2 = self.sqrt_num_patches//2 )\n","        embedding0 = rearrange(embedding0, 'b (p1 p2) c -> b c p1 p2', p1 = self.sqrt_num_patches, p2 = self.sqrt_num_patches )\n","       \n","        \n","        \n","        # temp1, temp2 = torch.split(embedding, [256,256], dim = 1)\n","        # embedding = self.skip4(embedding)\n","\n","        # temp1, temp2 = torch.split(embedding2, [128,128], dim = 1)\n","        embedding2 = self.skip3(embedding2)\n","\n","        # temp1, temp2 = torch.split(embedding1, [64,64], dim = 1)\n","        embedding1 = self.skip2(embedding1)\n","\n","        # temp1, temp2 = torch.split(embedding0, [32,32], dim = 1)\n","        embedding0 = self.skip1(embedding0)\n","\n","        x = self.du1(embedding)\n","        x = self.du2(x, embedding2, 16)\n","        x = self.du3(x, embedding1, 32)\n","        x = self.du4(x, embedding0, 64)\n","        x = self.conv_last(x)\n","\n","        return x"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zmtyIYh9wlEt"},"outputs":[],"source":["model = MLPMixer().cuda()"]},{"cell_type":"markdown","metadata":{"id":"RgTY707XdxZB"},"source":["# Loss and Metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-eSA9jNod2HY"},"outputs":[],"source":["from torch.nn.functional import cross_entropy\n","from torch.nn.modules.loss import _WeightedLoss"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VLP0LTw6gmLJ"},"outputs":[],"source":["import torch\n","from torch.nn.functional import cross_entropy\n","from torch.nn.modules.loss import _WeightedLoss\n","\n","\n","EPSILON = 1e-32\n","\n","\n","class LogNLLLoss(_WeightedLoss):\n","    __constants__ = ['weight', 'reduction', 'ignore_index']\n","\n","    def __init__(self, weight=None, size_average=None, reduce=None, reduction=None,\n","                 ignore_index=-100):\n","        super(LogNLLLoss, self).__init__(weight, size_average, reduce, reduction)\n","        self.ignore_index = ignore_index\n","\n","    def forward(self, y_input, y_target):\n","        # y_input = torch.log(y_input + EPSILON)\n","        return cross_entropy(y_input, y_target, weight=self.weight,\n","                             ignore_index=self.ignore_index)\n","\n","\n","def classwise_iou(output, gt):\n","    \"\"\"\n","    Args:\n","        output: torch.Tensor of shape (n_batch, n_classes, image.shape)\n","        gt: torch.LongTensor of shape (n_batch, image.shape)\n","    \"\"\"\n","    #dims = (0, *range(2, len(output.shape)))\n","    #gt = torch.zeros_like(output).scatter_(1, gt[:, None, :], 1)\n","    output = torch.argmax(output, dim=1)\n","    intersection = output*gt\n","    union = output + gt - intersection\n","    #classwise_iou = (intersection.sum(dim=dims).float() + EPSILON) / (union.sum(dim=dims) + EPSILON)\n","    classwise_iou = (intersection.sum().float() + EPSILON) / (union.sum() + EPSILON)\n","    return classwise_iou\n","\n","\n","def classwise_f1(output, gt):\n","    \"\"\"\n","    Args:\n","        output: torch.Tensor of shape (n_batch, n_classes, image.shape)\n","        gt: torch.LongTensor of shape (n_batch, image.shape)\n","    \"\"\"\n","\n","    epsilon = 1e-20\n","    n_classes = output.shape[1]\n","\n","    output = torch.argmax(output, dim=1)\n","    #print(output)\n","    true_positives = torch.tensor([((output == i) * (gt == i)).sum() for i in range(n_classes)]).float()\n","    true_positives = true_positives[1].item()\n","    selected = ((output == 1)).sum().float()\n","    relevant = ((gt == 1)).sum().float()\n","    #selected = torch.tensor([(output == i).sum() for i in range(n_classes)]).float()\n","    #relevant = torch.tensor([(gt == i).sum() for i in range(n_classes)]).float()\n","    #print(\"relevant:\",relevant)\n","    #print(\"selected:\",selected)\n","    \n","    precision = (true_positives + epsilon) / (selected + epsilon)\n","    recall = (true_positives + epsilon) / (relevant + epsilon)\n","    #print(precision)\n","    #print(recall)\n","    classwise_f1 = 2 * (precision * recall + EPSILON) / (precision + recall + EPSILON)\n","\n","    return classwise_f1\n","def classwise_dicescore(output, gt):\n","    \"\"\"\n","    Args:\n","        output: torch.Tensor of shape (n_batch, n_classes, image.shape)\n","        gt: torch.LongTensor of shape (n_batch, image.shape)\n","    \"\"\"\n","    epsilon = 1e-20\n","    n_classes = output.shape[1]\n","\n","    output = torch.argmax(output, dim=1)\n","    #print(output)\n","    true_positives = torch.tensor([((output == i) * (gt == i)).sum() for i in range(n_classes)]).float()\n","    true_positives = true_positives[1].item()\n","    selected = ((output == 1)).sum().float()\n","    relevant = ((gt == 1)).sum().float()\n","    dice_score = 2 * true_positives / (selected + relevant)\n","    return dice_score\n","\n","def make_weighted_metric(classwise_metric):\n","    \"\"\"\n","    Args:\n","        classwise_metric: classwise metric like classwise_IOU or classwise_F1\n","    \"\"\"\n","\n","    def weighted_metric(output, gt, weights=None):\n","\n","        # dimensions to sum over\n","        dims = (0, *range(2, len(output.shape)))\n","\n","        # default weights\n","        if weights == None:\n","            weights = torch.ones(output.shape[1]) / output.shape[1]\n","        else:\n","            # creating tensor if needed\n","            if len(weights) != output.shape[1]:\n","                raise ValueError(\"The number of weights must match with the number of classes\")\n","            if not isinstance(weights, torch.Tensor):\n","                weights = torch.tensor(weights)\n","            # normalizing weights\n","            weights /= torch.sum(weights)\n","\n","        classwise_scores = classwise_metric(output, gt).cpu()\n","\n","        return classwise_scores \n","\n","    return weighted_metric"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rUnKYh5PvpC"},"outputs":[],"source":["# Implement Focal Loss\n","class FocalLoss(nn.Module):\n","    def __init__(self, alpha=None, gamma=0, size_average=True,):\n","        super().__init__()\n","        self.gamma = gamma\n","        if isinstance(alpha, (float, int)): \n","            self.alpha = torch.tensor([alpha, 1-alpha])\n","        if isinstance(alpha, (list)) :\n","            self.alpha = torch.tensor(alpha)\n","        self.size_average = size_average\n","        #self.alpha = self.alpha.to(device)\n","    def forward(self, inputs, targets):\n","        \"\"\"\n","        Inputs:\n","        targets : shape (N, 1, H, W), dtype = long\n","        inputs : shape (N, C, H, W) - has propability for each class\n","        \n","        Returns:\n","        Focal loss between groundtruth and predict\n","        \"\"\"\n","        if inputs.dim() > 2:\n","            B, C, H, W = inputs.shape\n","            inputs = inputs.contiguous().permute(0,2,3,1) # shape (B, H, W, C)\n","            inputs = inputs.contiguous().reshape(B*H*W,C)\n","        targets = targets.reshape(-1, 1) # shape (N*H*W, 1)\n","        \n","        logpt = F.log_softmax(inputs, dim = 1)\n","        logpt = logpt.gather(1, targets)\n","        logpt = logpt.view(-1) # shape (N*H*W)\n","        pt = logpt.exp()\n","        #print(targets.device)\n","        if self.alpha is not None:\n","            if self.alpha.type() != inputs.data.type():\n","                self.alpha = self.alpha.to(inputs.dtype)\n","            self.alpha = self.alpha.to(inputs.device)\n","            at = self.alpha.gather(0, targets.view(-1))\n","            logpt = logpt * at\n","        loss = -1. * (1 - pt)**self.gamma * logpt\n","        if self.size_average:\n","            return loss.mean()\n","        else:\n","            return loss.sum()\n","        \n","        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LKt9SeDZPvpD"},"outputs":[],"source":["# Implement Focal Loss\n","class DiceLoss(nn.Module):\n","    def __init__(self, size_average=True,):\n","        super().__init__()\n","    def forward(self, inputs, targets):\n","        \"\"\"\n","        Inputs:\n","        targets : shape (N, 1, H, W), dtype = long\n","        inputs : shape (N, C, H, W) - has propability for each class\n","        \n","        Returns:\n","        Focal loss between groundtruth and predict\n","        \"\"\"\n","        if inputs.dim() > 2:\n","            B, C, H, W = inputs.shape\n","            inputs = inputs.contiguous().permute(0,2,3,1) # shape (B, H, W, C)\n","            inputs = inputs.contiguous().reshape(B*H*W,C)\n","        targets = targets.reshape(-1, 1) # shape (N*H*W, 1)\n","        \n","        logpt = F.log_softmax(inputs, dim = 1)\n","        logpt = logpt.gather(1, targets)\n","        logpt = logpt.view(-1) # shape (N*H*W)\n","        pt = logpt.exp()\n","        #print(targets.device)\n","        pt = pt.view(-1) # shape (N*H*W)\n","        intersection = (pt * targets.view(-1)).sum()\n","        #print(targets.device)\n","        dice = (2. * intersection + 1e-32) / (pt.sum() + targets.sum() + 1e-32)\n","        return 1 - dice        \n","        "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ffvgs481PvpD"},"outputs":[],"source":["class CEDiceloss(nn.Module):\n","    def __init__(self, alpha = 0.5):\n","        super().__init__()\n","        self.alpha = alpha\n","    def forward(self, inputs, targets):\n","        criterion1 = DiceLoss()\n","        criterion2 = nn.CrossEntropyLoss()\n","        return self.alpha * criterion1(inputs, targets) + (1 - self.alpha) * criterion2(inputs, targets)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1673960638075,"user":{"displayName":"Minh Quang Lê Hoàng","userId":"07289054064054334657"},"user_tz":-420},"id":"d6oubRFbPvpE","outputId":"1ad8785f-45b0-497b-faa1-b1b4c18651a7"},"outputs":[{"data":{"text/plain":["tensor(0.2562, device='cuda:0')"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["criterion = FocalLoss(0.4, 0.8)\n","inputs_test = torch.rand(1, 2, 128, 128).to(\"cuda\")\n","#pred_test = torch.randint(0, 1, size=(1, 128, 128)).to(\"cuda\")\n","pred_test = torch.ones(1,128,128, dtype=torch.int64).to(\"cuda\")\n","criterion(inputs_test, pred_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":21,"status":"ok","timestamp":1673960638076,"user":{"displayName":"Minh Quang Lê Hoàng","userId":"07289054064054334657"},"user_tz":-420},"id":"iEXIf5fhgdtL","outputId":"09449776-e281-4661-b6bb-dbf43027bd19"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor(1.)\n"]}],"source":["#output, gt = torch.ones(3, 2, 5, 5), torch.ones(3, 5, 5).long()\n","gt = torch.ones(3, 5, 5).long()\n","output = torch.stack([torch.zeros(3,5,5), torch.ones(3,5,5)], dim=1)\n","print(classwise_dicescore(output, gt))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1673960638076,"user":{"displayName":"Minh Quang Lê Hoàng","userId":"07289054064054334657"},"user_tz":-420},"id":"4TG4PMpqTYnp","outputId":"db08fea5-f05d-4c51-ef9e-720a8cb74871"},"outputs":[{"data":{"text/plain":["tensor(1.)"]},"execution_count":33,"metadata":{},"output_type":"execute_result"}],"source":["classwise_iou(output, gt)\n","#print(output.shape)"]},{"cell_type":"markdown","metadata":{"id":"dtsRItgkhaEI"},"source":["# Train and Test"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m8QYEeuehdrI"},"outputs":[],"source":["from tqdm import tqdm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hddwVtAriFRh"},"outputs":[],"source":["lr = 1e-3\n","DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","NUM_EPOCHS = 200\n","IMG_SIZE = 256\n","IMG_CHANEL = 3\n","save_freq = 1000"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ziziAtm3s21g"},"outputs":[],"source":["# def save_checkpoint(state, filename=\"my_checkpoint_ceil.pth.tar\"):\n","#     print(\"=>Saving checkpoint\")\n","#     torch.save(state, filename)\n","# def load_checkpoint(checkpoint, model):\n","#     print(\"=> Loading checkpoint\")\n","#     model.load_state_dict(checkpoint[\"state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n_18V-lejGo1"},"outputs":[],"source":["def save_checkpoint(state, filename=\"my_checkpoint_ceil_1.pth.tar\"):\n","    print(\"=>Saving checkpoint\")\n","    torch.save(state, filename)\n","def load_checkpoint(checkpoint, model):\n","    print(\"=> Loading checkpoint\")\n","    model.load_state_dict(checkpoint[\"state_dict\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WYgQ97srPvpH"},"outputs":[],"source":["# model = Permute_Unet(image_size=256,patch_size=8, depth=24, segments=16, dim = 256)\n","# model.cuda();"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CFPWpcPchjP8"},"outputs":[],"source":["criterion = CEDiceloss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=1e-3)\n","scaler = torch.cuda.amp.GradScaler()\n","swa_model = torch.optim.swa_utils.AveragedModel(model)\n","swa_scheduler = torch.optim.swa_utils.SWALR(optimizer, anneal_strategy=\"linear\", anneal_epochs=5, swa_lr=1e-5)\n","swa_model.cuda()\n","swa_start = 2000\n","#criterion = LogNLLLoss()\n","reset_seed()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X-KgPcPAPvpH"},"outputs":[],"source":["#optimizer_1 = torch.optim.Adam(model_fake.parameters(), lr=1e-4, weight_decay=1e-4)\n","#swa_scheduler = torch.optim.swa_utils.SWALR(optimizer_1, swa_lr=1e-4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GL2oqf5jTn4"},"outputs":[],"source":["def train_fn(loader, model,optimizer, loss_fn, scaler):\n","    \n","    model.train()\n","    train_running_loss = 0\n","    my_f1 = 0\n","    my_iou = 0\n","    counter = 0\n","    loop = tqdm(loader)\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.long().to(DEVICE)\n","        # forward\n","        with torch.cuda.amp.autocast():\n","            prediction = model(data)\n","            loss = loss_fn(prediction, targets)\n","        tmp = prediction.detach().cpu()\n","        tmp2 = targets.detach().cpu()\n","        my_f1 += classwise_f1(tmp, tmp2).item()\n","        my_iou += classwise_iou(tmp, tmp2).item()\n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","        # update tqdm loop\n","        loop.set_postfix(loss = loss.item())\n","        train_running_loss += loss.item()\n","        counter += 1\n","    return train_running_loss / counter, my_f1/counter, my_iou/counter\n","\n","def check_accuracy(loader, model, loss_fn, device=\"cuda\"):\n","    my_f1 = 0\n","    my_iou = 0\n","    my_dicescore = 0\n","    val_running_loss = 0\n","    model.eval()\n","    with torch.no_grad():\n","        for X, y in loader:\n","            batch_size = X.shape[0]\n","            X = X.to(device)\n","            y = y.long().to(device)\n","            preds = model(X)\n","            loss = loss_fn(preds, y)\n","            val_running_loss += loss.item()\n","            tmp = preds.detach().cpu()\n","            tmp2 = y.detach().cpu()\n","            my_f1 += classwise_f1(tmp, tmp2).item()\n","            my_iou += classwise_iou(tmp, tmp2).item()\n","            #my_dicescore += classwise_dicescore(tmp, tmp2).item()\n","            #my_dicescore += classwise_dicescore(tmp, tmp2).item()\n","    model.train()\n","    print(f\"IoU score: {my_iou/len(loader)}\")\n","    print(f\"F1 score: {my_f1/len(loader)} \")\n","    return val_running_loss/len(loader),my_f1/len(loader),my_iou/len(loader)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mrswdirTPvpI"},"outputs":[],"source":["def train_fn_swa(loader, model, swa_model, optimizer, loss_fn, scaler):\n","    model.train()\n","    train_running_loss = 0\n","    my_f1 = 0\n","    my_iou = 0\n","    counter = 0\n","    loop = tqdm(loader)\n","    for batch_idx, (data, targets) in enumerate(loop):\n","        data = data.to(device=DEVICE)\n","        targets = targets.long().to(DEVICE)\n","\n","        # forward\n","\n","        with torch.cuda.amp.autocast():\n","            prediction = model(data)\n","            loss = loss_fn(prediction, targets)\n","        \n","        tmp = prediction.detach().cpu()\n","        tmp2 = targets.detach().cpu()\n","        my_f1 += classwise_f1(tmp, tmp2).item()\n","        my_iou += classwise_iou(tmp, tmp2).item()\n","        \n","        # backward\n","        optimizer.zero_grad()\n","        scaler.scale(loss).backward()\n","        scaler.step(optimizer)\n","        scaler.update()\n","\n","        # update tqdm loop\n","        loop.set_postfix(loss = loss.item())\n","        train_running_loss += loss.item()\n","        counter += 1\n","    swa_model.update_parameters(model)\n","    swa_scheduler.step()\n","    bn_update(swa_model, train_loader)\n","    return train_running_loss / counter, my_f1/counter, my_iou/counter\n","\n","# batchnormalize update running mean + running var\n","def bn_update(swa_model, loader):\n","    with torch.no_grad():\n","        for data, target in loader:\n","            data = data.to(device=DEVICE)\n","            #target.to(device=DEVICE)\n","            _ = swa_model(data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8Nk-YchnPvpI"},"outputs":[],"source":["loss_history = {\"train\":[],\"test\":[]}\n","acc_history = {\"train_f1\":[],\"test_f1\":[],\"train_iou\":[],\"test_iou\":[]}\n","checkpoint_history = {}\n","best_accuracy = 0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fyvLox2fPvpI"},"outputs":[],"source":["def train_normal():\n","    for epoch in range(0, 300):\n","\n","        #model.cuda()\n","        print(f\"=============Epoch {epoch}============>\")\n","        if (epoch < swa_start) :\n","            train_loss,train_f1,train_iou = train_fn(train_loader, model, optimizer, criterion, scaler)\n","        else :\n","            train_loss,train_f1,train_iou = train_fn_swa(train_loader, model, swa_model, optimizer, criterion, scaler)\n","        if epoch == 10:\n","            for param in model.parameters():\n","                param.requires_grad =True\n","\n","\n","        if (epoch % save_freq) == 0:\n","            if (epoch >=  swa_start):\n","                checkpoint = {\n","                    \"state_dict\": swa_model.module.state_dict(),\n","                    \"optimizer\" : optimizer.state_dict(),\n","                    \"epoch\" : epoch + 1,\n","                }\n","                #checkpoint_history[\"epoch \"+str(epoch+1)] = checkpoint\n","                save_checkpoint(checkpoint)\n","                print(\"test score:\")\n","                test_loss,test_f1,test_iou = check_accuracy(test_loader, swa_model, criterion, DEVICE)\n","                if test_f1 > best_accuracy:\n","                    best_accuracy = test_f1\n","                    best_checkpoint = epoch\n","                    name = f\"dice score {test_f1:.4f},test_score {test_iou:.4f},epoch {epoch}\".replace(\".\",\",\")\n","                    save_checkpoint(checkpoint, filename=name+\".pth.tar\")\n","                print(\"train score:\")\n","                train_loss, train_f1, train_iou = check_accuracy(train_loader, swa_model, criterion, DEVICE)\n","                print(\"train score:\")\n","                loss_history[\"test\"].append(test_loss)\n","                loss_history[\"train\"].append(train_loss)\n","                acc_history[\"train_f1\"].append(train_f1)\n","                acc_history[\"train_iou\"].append(train_iou)\n","                acc_history[\"test_f1\"].append(test_f1)\n","                acc_history[\"test_iou\"].append(test_iou)\n","            else :\n","                checkpoint = {\n","                    \"state_dict\": model.state_dict(),\n","                    \"optimizer\" : optimizer.state_dict(),\n","                    \"epoch\" : epoch + 1,\n","                }\n","                #checkpoint_history[\"epoch \"+str(epoch+1)] = checkpoint\n","                save_checkpoint(checkpoint)\n","                print(\"test score:\")\n","                test_loss,test_f1,test_iou = check_accuracy(test_loader, model, criterion, DEVICE)\n","                if test_f1 > best_accuracy:\n","                    best_accuracy = test_f1\n","                    best_checkpoint = epoch\n","                    name = f\"dice score {test_f1:.4f},test_score {test_iou:.4f},epoch {epoch}\".replace(\".\",\",\")\n","                    save_checkpoint(checkpoint, filename=name+\".pth.tar\")\n","                print(\"train score:\")\n","                train_loss, train_f1, train_iou = check_accuracy(train_loader, model, criterion, DEVICE)\n","                print(\"train score:\")\n","                loss_history[\"test\"].append(test_loss)\n","                loss_history[\"train\"].append(train_loss)\n","                acc_history[\"train_f1\"].append(train_f1)\n","                acc_history[\"train_iou\"].append(train_iou)\n","                acc_history[\"test_f1\"].append(test_f1)\n","                acc_history[\"test_iou\"].append(test_iou)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mXVRPA3PvpJ"},"outputs":[],"source":["def train_eff():\n","    best_accuracy = 0\n","    for epoch in range(0, 300):\n","\n","        #model.cuda()\n","        print(f\"=============Epoch {epoch}============>\")\n","        if (epoch < swa_start) :\n","            train_loss,train_f1,train_iou = train_fn(train_loader, model, optimizer, criterion, scaler)\n","        else :\n","            train_loss,train_f1,train_iou = train_fn_swa(train_loader, model, swa_model, optimizer, criterion, scaler)\n","        print(\"train score:\")\n","        print(f\"IoU score: {train_iou}\")\n","        print(f\"Dice score: {train_f1}\")\n","        if epoch == 10:\n","            for param in model.parameters():\n","                param.requires_grad =True\n","\n","\n","\n","        if (epoch >=  swa_start):\n","            #checkpoint_history[\"epoch \"+str(epoch+1)] = checkpoint\n","            if (epoch % 1000 == 0 and epoch > 0):\n","                save_checkpoint(checkpoint,filename=f\"epoch {epoch}.pth.tar\")\n","            print(\"test score:\")\n","            test_loss,test_f1,test_iou = check_accuracy(test_loader, swa_model, criterion, DEVICE)\n","            if test_f1 > best_accuracy:\n","                best_accuracy = test_f1\n","                checkpoint = {\n","                \"state_dict\": swa_model.module.state_dict(),\n","                \"epoch\" : epoch + 1,\n","            }\n","                name = f\"dice score {test_f1:.4f},test_score {test_iou:.4f},epoch {epoch}\".replace(\".\",\",\")\n","                save_checkpoint(checkpoint, filename=name+\".pth.tar\")\n","            loss_history[\"test\"].append(test_loss)\n","            loss_history[\"train\"].append(train_loss)\n","            acc_history[\"train_f1\"].append(train_f1)\n","            acc_history[\"train_iou\"].append(train_iou)\n","            acc_history[\"test_f1\"].append(test_f1)\n","            acc_history[\"test_iou\"].append(test_iou)\n","        else :\n","            #checkpoint_history[\"epoch \"+str(epoch+1)] = checkpoint\n","            if (epoch % 1000 == 0 and epoch > 0):\n","                save_checkpoint(checkpoint,filename = f\"epoch {epoch}.pth.tar\")\n","            print(\"test score:\")\n","            test_loss,test_f1,test_iou = check_accuracy(test_loader, model, criterion, DEVICE)\n","            if test_f1 > best_accuracy:\n","                best_accuracy = test_f1\n","                checkpoint = {\n","                \"state_dict\": model.state_dict(),\n","                \"epoch\" : epoch + 1,\n","            }\n","                name = f\"dice score {test_f1:.4f},test_score {test_iou:.4f},epoch {epoch}\".replace(\".\",\",\")\n","                save_checkpoint(checkpoint, filename=name+\".pth.tar\")\n","            loss_history[\"test\"].append(test_loss)\n","            loss_history[\"train\"].append(train_loss)\n","            acc_history[\"train_f1\"].append(train_f1)\n","            acc_history[\"train_iou\"].append(train_iou)\n","            acc_history[\"test_f1\"].append(test_f1)\n","            acc_history[\"test_iou\"].append(test_iou)\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.7 ('base')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"2043299c89c8cd0b4d1a6f5cf4529bd58e6a4e0fe3181a25e0d328c821cdc5c5"}}},"nbformat":4,"nbformat_minor":0}
